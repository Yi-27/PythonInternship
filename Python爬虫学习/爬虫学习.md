# 认识爬虫与反爬虫

## 浏览网页基本流程：

+ 浏览器**发送请求**给服务器
+ 服务器**返回响应**内容给浏览器



## 爬虫：

网络爬虫也被称为网络蜘蛛、网络机器人，是一个自动下载网页的计算机程序或自动化脚本。

网络爬虫就像一只蜘蛛一样在互联网上沿着URL的丝线爬行，下载每一个URL所指向的网页，分析页面内容。



### 通用网络爬虫

​	通用网络爬虫又称为全网爬虫，其爬行对象由一批种子URL扩充至整个Web，该类爬虫比较适合为搜索引擎搜索广泛的主题，主要由搜索引擎或大型Web服务提供商使用。

+ **深度优先策略：**按照深度由低到高的顺序，依次访问下一级网页链接，直到无法再深入为止。

+ **广度优先策略：**按照网页内容目录层次的深浅来爬行，优先爬取较浅层次的页面。当同一层中的页面全部爬行完毕后，爬虫再深入下一层。



### 聚焦网络爬虫

​	聚焦网络爬虫又被称作主题网络爬虫，其最大的特点是只选择性地爬行与预设的主题相关的页面。

+  **基于内容评价的爬行策略：**该种策略将用户输入的查询词作为主题，包含查询词的页面被视为与主题相关的页面。

+ **基于链接结构评价的爬行策略：**该种策略将包含很多结构信息的半结构化文档Web页面用来评价链接的重要性，其中一种广泛使用的算法为PageRank算法。

+ **基于增强学习的爬行策略：**该种策略将增强学习引入聚焦爬虫，利用贝叶斯分类器对超链接进行分类，计算出每个链接的重要性，按照重要性决定链接的访问顺序。

+ **基于语境图的爬行策略：**该种策略通过建立语境图学习网页之间的相关度，计算当前页面到相关页面的距离，距离越近的页面中的链接优先访问。



### 增量式网络爬虫

​	增量式网络爬虫只对已下载网页采取增量式更新或只爬行新产生的及已经发生变化的网页，需要通过重新访问网页对本地页面进行更新，从而保持本地集中存储的页面为最新页面。

​	常用的更新方法如下。

+ **统一更新法：**以相同的频率访问所有网页，不受网页本身的改变频率的影响。

+ **个体更新法：**根据个体网页的改变频率来决定重新访问各页面的频率。

+ **基于分类的更新法：**爬虫按照网页变化频率分为更新较快和更新较慢的网页类别，分别设定不同的频率来访问这两类网页。



### 深层网络爬虫

​	Web页面按照存在方式可以分为表层页面和深层页面两类。表层页面指以传统搜索引擎可以索引到的页面，深层页面为大部分内容无法通过静态链接获取，隐藏在搜索表单后的，需要用户提交关键词后才能获得的Web页面。

​	深层爬虫的核心部分为表单填写，包含以下两种类型。

+ **基于领域知识的表单填写：**该种方法一般会维持一个本体库，通过语义分析来选取合适的关键词填写表单。

+ **基于网页结构分析的表单填写：**这种方法一般无领域知识或仅有有限的领域知识，将HTML网页表示为DOM树形式，将表单区分为单属性表单和多属性表单，分别进行处理，从中提取表单各字段值。



## 爬虫的合法性与robots协议

### 爬虫的合法性

​	目前，多数网站允许将爬虫爬取的数据用于个人使用或者科学研究。但如果将爬取的数据用于其他用途，尤其是转载或者商业用途，严重的将会触犯法律或者引起民事纠纷。

​	以下两种数据是不能爬取的，更不能用于商业用途。

+ **个人隐私数据：**如姓名、手机号码、年龄、血型、婚姻情况等，爬取此类数据将会触犯个人信息保护法。

+ **明确禁止他人访问的数据：**例如用户设置了账号密码等权限控制，进行了加密的内容。

    

    还需注意版权相关问题，有作者署名的受版权保护的内容不允许爬取后随意转载或用于商业用途。



### robots协议

+ robots协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（ Robots Exclusion Protocol ），当使用一个爬虫爬取一个网站的数据时，需要遵守网站所有者针对所有爬虫所制定的协议。

+ 它通常是一个叫作robots.txt的文本文件，该协议通常存放在网站根目录下，里面规定了此网站哪些内容可以被爬虫获取，及哪些网页是不允许爬虫获取的。

+ robots.txt 的样例

    + ```
        User-agent: * 
        Disallow: /
        Allow: /public/
        ```

        + 这实现了对所有搜索爬虫只允许爬取public目录的功能，将上述内容保存成robots.txt文件，放在网站的根目录下，和网站的入口文件（比如index.php、index.html和index.jsp等）放在一起。 
        + 上面的User-agent描述了搜索爬虫的名称，这里将其设置为＊则代表该协议对任何爬取爬虫有效。 比如，我们可以设置：User-agent: Baiduspider 。这就代表我们设置的规则对百度爬虫是有效的。 如果有多条User-agent记录，则就会有多个爬虫会受到爬取限制，但至少需要指定一条。 Disallow 指定了不允许抓取的目录，比如上例子中设置为／则代表不允许抓取所有页面。Allow一般和Disallow一起使用，一般不会单独使用，用来排除某些限制。 现在我们设置为/public／，则表示所有页面不允许抓取，但可以抓取public目录。

    + 禁止所有爬虫访问任何目录的代码如下:

    + ```
        User-agent: * 
        Disallow: /
        ```

    + 允许所有爬虫访问任何目录的代码如下：

    + ```
        User-agent: * 
        Disallow: 
        ```

    + 禁止所有爬虫访问网站某些目录的代码如下：

    + ```
        User-agent: * 
        Disallow: /private/ 
        Disallow: /tmp/
        ```

    + 只允许某一个爬虫访问的代码如下：

    + ```
        User-agent: WebCrawler
        Disallow: 
        User-agent: * 
        Disallow: /
        ```



### robotparser模块

+ 了解 Robots 协议之后，我们就可以使用robotparser模块来解析robots.txt了。 该模块提供了一个类RobotFileParser，它可以根据某网站的robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。 

+ 该类用起来非常简单，只需要在构造方法里传入robots.txt的链接即可。 首先看一下它的声明：urllib.robotparser.RobotFileParser(url=’’)，当然也可以在声明时不传入，默认为空，最后再使用 set_url（）方法设置一下也可。 
+ 下面列举了这个类常用的几个方法。 
    + set_url（）：用来设置robots.txt文件的链接。 如果在创建RobotFileParser对象时传入了链接，那么就不需要再使用这个方法设置了。 
    + read（）：读取robots.txt文件并进行分析。 注意，这个方法执行一个读取和分析操作，如果不调用这个方法， 接下来的判断都会为False，所以一定记得调用这个方法。 这个方法不会返 回任何内容，但是执行了读取操作。 
    +  parse（）：用来解析robots.txt文件，传人的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。 
    + can_fetch（）：该方法传入两个参数， 第一个是User-agent，第二个是要抓取的URL。 返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是True或False。
    + mtime（）：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的robots.txt。 
    + modified（） ：它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取 和分析robots.txt的时间。



## 网站反爬虫的目的与手段

### 通过User-Agent校验反爬

​	浏览器在发送请求的时候，会附带一部分浏览器及当前系统环境的参数给服务器，服务器会通过User-Agent的值来区分不同的浏览器。



### 通过访问频度反爬

+ 普通用户通过浏览器访问网站的速度相对爬虫而言要慢的多，所以不少网站会利用这一点对访问频度设定一个阈值，如果一个IP单位时间内访问频度超过了预设的阈值，将会对该IP做出访问限制。

+ 通常需要经过验证码验证后才能继续正常访问，严重的甚至会禁止该IP访问网站一段时间。



### 通过验证码校验反爬

​	有部分网站不论访问频度如何，一定要来访者输入验证码才能继续操作。例如12306网站，不管是登陆还是购票，全部需要验证验证码，与访问频度无关。



### 通过变换网页结构反爬

​	一些社交网站常常会更换网页结构，而爬虫大部分情况下都需要通过网页结构来解析需要的数据，所以这种做法也能起到反爬虫的作用。在网页结构变换后，爬虫往往无法在原本的网页位置找到原本需要的内容。



### 通过账号权限反爬

+ 部分网站需要登录才能继续操作，这部分网站虽然并不是为了反爬虫才要求登录操作，但确实起到了反爬虫的作用。

+ 例如微博查看评论就需要登录账号。



## 爬取策略制定

​	针对之前介绍的常见的反爬虫手段，可以制定对应的爬取策略如下。

+ **发送模拟User-Agent**：通过发送模拟User-Agent来通过检验，将要发送至网站服务器的请求的User-Agent值伪装成一般用户登录网站时使用的User-Agent值。

+ **调整访问频度：**通过备用IP测试网站的访问频率阈值，然后设置访问频率比阈值略低。这种方法既能保证爬取的稳定性，又能使效率又不至于过于低下。

+ **通过验证码校验：**使用IP代理，更换爬虫IP；通过算法识别验证码；使用cookie绕过验证码。

+ **应对网站结构变化：**只爬取一次时，在其网站结构调整之前，将需要的数据全部爬取下来；使用脚本对网站结构进行监测，结构变化时，发出告警并及时停止爬虫。

+ **通过账号权限限制：**通过模拟登录的方法进行规避，往往也需要通过验证码检验。

+ **通过代理IP规避**：通过代理进行IP更换可有效规避网站检测，需注意公用IP代理池是 网站重点监测对象。



## 可抓取的数据

+ 网页文本：如HTML文档、Json格式文本等。

+ 图片：获取到的是二进制文件，保存为图片格式。

+ 视频：同为二进制文件，保存为视频格式即可。

+ 其他：只要是能请求到的，都能获取。





# Python爬虫相关库介绍与配置

## Python爬虫相关库

​	目前Python有着形形色色的爬虫相关库，按照库的功能，整理如下。

+ 类型——库名——简介
    + **通用**
        + urllib：Python内置的HTTP请求库，提供一系列用于操作URL的功能
        + requests：基于urllib,采用Apache2 Licensed开源协议的HTTP库
        + urllib 3：提供很多Python标准库里所没有的重要特性:线程安全，连接池，客户端SSL/TLS验证，文件分部编码上传，协助处理重复请求和HTTP重定位，支持压缩编码，支持HTTP和SOCKS代理，100%测试覆盖率
    + **框架**
        + scrapy：一个为了爬取网站数据，提取结构性数据而编写的应用框架
    + **HTML/XML**解析器
        + lxml：C语言编写高效HTML/XML处理库。支持XPath解析器
        + BeautifulSoup 4：纯Python实现的HTML/XML处理库， 效率相对较低





# 认识HTTP协议

## HTTP请求方式与过程

爬虫在爬取数据时将会作为客户端模拟整个HTTP通信过程，该过程也需要通过HTTP协议实现。HTTP请求过程如下。

+ 由HTTP客户端向服务器发起一个请求，创建一个到服务器指定端口（默认是80端口）的TCP连接。

+ HTTP服务器从该端口监听客户端的请求。

+ 一旦收到请求，服务器会向客户端返回一个状态，比如“HTTP/1.1 200 OK”，以及返回的响应内容，如请求的文件、错误消息、或其它信息。



### 请求方法

+ 在HTTP/1.1协议中共定义了8种方法（也叫“动作”）来以不同方式操作指定的资源，常用方法有GET、HEAD、POST等。

+ 请求方法——方法描述
    + GET：请求指定的页面信息，并返回实体主体。GET可能会被网络爬虫等随意访问，因此GET方法应该只用在读取数据，而不应当被用于产生“副作用”的操作中，例如在Web Application中
    + HEAD：与GET方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回具体的内容，使用这个方法可以在不必传输全部内容的情况下，就可以获取其中该资源的相关信息(元信息或称元数据
    + POST：向指定资源提交数据，请求服务器进行处理(例如提交表单或者上传文件)。数据会被包含在请求中，这个请求可能会创建新的资源或修改现有资源，或二者皆有
    + PUT：从客户端上传指定资源的最新内容，即更新服务器端的指定资源。



### 请求（request）与响应（response）

HTTP协议采用了请求／响应模型。

+ 客户端向服务器发送一个请求报文，请求报文包含请求的方法、URL、协议版本、请求头部和请求数据。

+ 服务器以一个状态行作为响应，响应的内容包括协议的版本、响应状态、服务器信息、响应头部和响应数据。

+ 客户端与服务器间的请求与响应的具体步骤如下。
    + **连接Web服务器**：由一个HTTP客户端发起连接，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接。
    + **发送HTTP请求**：客户端经TCP套接字向Web服务器发送一个文本的请求报文。
    + **服务器接受请求并返回HTTP响应**：Web服务器解析请求，定位该次的请求资源。之后将资源复本写至TCP套接字，由客户端进行读取。
    + **释放连接TCP连接**：若连接的connection模式为close，则由服务器主动关闭TCP连接，客户端将被动关闭连接，释放TCP连接；若connection模式为keepalive，则该连接会保持一段时间。
    + **客户端解析HTML内容**：客户端首先会对状态行进行解析，之后解析每一个响应头，最后读取响应数据。



### Request

+ 请求方式：主要有GET、POST两种类型，另外还有HEAD、PUT、DELETE、OPTIONS等。

+ 请求URL：URL全称统一资源定位符，如一个网页文档、一张图片、一个视频等都可以用URL唯一确定。

+ 请求头：包含请求时的头部信息，如User-Agent、Host、Cookies等信息。

+ 请求体：请求时额外携带的数据，如表单提交时的表单数据



### Response

+ 响应状态：有多种响应状态，如200代表成功、301跳转、404找不到页面、502服务器错误

+ 响应头：如内容类型、内容长度、服务器信息、设置Cookie等等。

+ 响应体：最主要的部分，包含了请求资源的内容，如网页HTML、图片二进制数据等。



## 常见HTTP状态码

### HTTP状态码种类

​	HTTP状态码是用来表示网页服务器响应状态的3位数字代码，按首位数字分为5类状态码。

+ 状态码类型——状态码意义
    + 1XX：表示请求已被接受，需接后续处理。这类响应是临时响应，只包含状态行和某些可选的响应头信息，并以空行结束
    + 2XX：表示请求已成功被服务器接收、理解并接受
    + 3XX：表示需要客户端采取进一-步的操作才 能完成请求。通常用来重定向，重定向目标需在本次响应中指明
    + 4XX：表示客户端可能发生了错误，妨碍了服务器的处理。
    + 5XX：表示服务器在处理请求的过程中有错误或者异常状态发生，也有可能是服务器以当前的软硬件资源无法完成对请求的处理。



### 常见HTTP状态码

​	HTTP状态码共有67种状态码，常见的状态码如下。

+ 常见状态码——状态码含义
    + **200 OK**：请求成功，请求所希望的响应头或数据体将随此响应返回。
    + **400 Bad Request**：由于客户端的语法错误、无效的请求或欺骗性路由请求，服务器不会处理该请求
    + **403 Forbidden**：服务器已经理解该请求，但是拒绝执行，将在返回的实体内描述拒绝的原因，也可以不描述仅返回404响应
    + **404 Not Found**：请求失败，请求所希望得到的资源未被在服务器上发现，但允许用户的后续请求
    + **500 Internal Server Error**：通用错误消息，服务器遇到了一一个未曾预料的状况，导致了它无法完成对请求的处理，不会给出具体错误信息
    + **503 Service Unavailable**：由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是暂时的，并且将在一段时间以后恢复



## HTTP头部信息

​	HTTP头部信息（HTTP header fields）是指在超文本传输协议（HTTP）的请求和响应消息中的消息头部分。头部信息定义了一个超文本传输协议事务中的操作参数。在爬虫中需要使用头部信息向服务器发送模拟信息，通过发送模拟的头部信息将自己伪装成一般的客户端。



### HTTP头部类型

​	HTTP头部类型按用途可分为：通用头，请求头，响应头，实体头。

+ **通用头：**既适用于客户端的请求头，也适用于服务端的响应头。与HTTP消息体内最终传输的数据是无关的，只适用于要发送的消息。

+ **请求头：**提供更为精确的描述信息，其对象为所请求的资源或请求本身。新版HTTP增加的请求头不能在更低版本的HTTP中使用，但服务器和客户端若都能对相关头进行处理，则可以在请求中使用。

+ **响应头：**为响应消息提供了更多信息。例如，关于资源位置的描述Location字段，以及关于服务器本身的描述使用Server字段等。与请求头类似，新版增加的响应头也不能在更低版本的HTTP版本中使用。

+ **实体头：**提供了关于消息体的描述。如消息体的长度Content-Length，消息体的MIME类型Content-Type。新版的实体头可以在更低版本的HTTP版本中使用。



+ 字段名——说明——示例
    + Accept：可接受的响应内容类型(Content-Types)
        + Accept: text/plain 
    + Accept-Charset：可接受的字符集
        + Accept-Charset:utf-8
    + Accept-Encoding：可接受的响应内容的编码方式
        + Accept- Encoding:gzip,deflate
    + Accept-Language：可接受的响应内容语言列表
        + Accept-Language:en-US
    + Cookie：由之前服务器通过Set-Cookie设置的一个HTTP协议Cookie
        + Cookie:$Version= 1;Skin=new;
    + Referer：设置前一个页面的地址，并且前一个页面中的连接指向当前请求，意思就是如果当前请求是在A页面中发送的，那么referer就是A页面的ur!地址
        + Referer:http://zh.wikipedia.org/wiki/Main_Page
    + User-Agent：用户代理的字符串值
        + User-Agent:Mozilla/5.0(X11;Linuxx86_ 64;rv:12.0)Gecko/201 00101 Firefox/21.0



## 熟悉Cookie

+ HTTP是一种无状态的协议，客户端与服务器建立连接并传输数据，在数据传输完成后，本次的连接将会关闭，并不会留存相关记录。

+ 服务器无法依据连接来跟踪会话，也无法从连接上知晓用户的历史操作。这严重阻碍了基于Web应用程序的交互，也影响用户的交互体验。

+ 某些网站需要用户登录才进一步操作，用户在输入账号密码登录后，才能浏览页面。对于服务器而言，由于HTTP的无状态性，服务器并不知道用户有没有登录过，当用户退出当前页面访问其他页面时，又需重新再次输入账号及密码。



### Cookie机制

​	为解决HTTP的无状态性带来的负面作用，Cookie机制应运而生。Cookie本质上是一段文本信息。

+ 当客户端请求服务器时，若服务器需要记录用户状态，就在响应用户请求时发送一段Cookie信息。

+ 客户端浏览器会保存该Cookie信息，当用户再次访问该网站时，浏览器会把Cookie做为请求信息的一部分提交给服务器。

+ 服务器对Cookie进行验证，以此来判断用户状态，当且仅当该Cookie合法且未过期时，用户才可直接登录网站。



### Cookie的存储方式

​	Cookie由用户客户端浏览器进行保存，按其存储位置可分为内存式存储和硬盘式存储。

+ 内存式存储将Cookie保存在内存中，在浏览器关闭后就会消失，由于其存储时间较短，因此也被称为非持久Cookie或会话Cookie。

+ 硬盘式存储将Cookie保存在硬盘中，其不会随浏览器的关闭而消失，除非用户手工清理或到了过期时间。由于硬盘式Cookie存储时间是长期的，因此也被称为持久Cookie。



### Cookie的实现过程

​	客户端与服务器间的Cookie实现过程的具体步骤如下。

+ **客户端请求服务器：**客户端请求网站页面

+ **服务器响应请求：**Cookie是一种字符串，为key=value形式，服务器需要记录这个客户端请求的状态，在响应头中增加一个Set-Cookie字段。

+ **客户端再次请求服务器：**客户端会对服务器响应的Set-Cookie头信息进行存储。当再次请求时，将会在请求头中包含服务器响应的Cookie信息。





# 静态网页的爬取

+ 在网站设计中，纯粹HTML（标准通用标记语言下的一个应用）格式的网页通常被称为“静态网页”，静态网页是标准的HTML文件，它的文件扩展名是.htm、.html，可以包含文本、图像、声音、FLASH动画、客户端脚本和ActiveX控件及JAVA小程序等。

+ 静态网页是网站建设的基础，早期的网站一般都是由静态网页制作的。静态网页是相对于动态网页而言，是指**没有后台数据库、不含程序和不可交互**的网页。



## 爬虫基本流程

+ 1、发起请求：通过HTTP库向目标站点发起请求，即发送一个Request，请求可以包含额外的headers等信息，等待服务器响应。

+ 2、获取响应内容：如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能有HTML，Json字符串，二进制数据（如图片视频）等类型。

+ 3、解析内容：得到的内容可能是HTML，可以用正则表达式、网页解析库进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。

+ 4、保存数据：保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。



## 使用urllib3库实现

​	许多Python的原生系统已经开始使用urllib3库，其提供了很多python标准库里所没有的重要特性。

+ 连接特性
    + 线程安全
    + 管理连接池
    + 客户端SSL/TLS验证
    + 使用分部编码.上传文件
    + 协助处理重复请求和HTTP重定位
    + 支持压缩编码
    + 支持HTTP和SOCKS代理测试覆盖率达到100%



### 生成请求

+ 通过request方法即可创建一个请求，该方法返回一个HTTP响应对象。Reques语法格式如下。

+ urllib3.request(method,url,fields=None,headers=None,**urlopen_kw)

+ 参数——说明

    + method：接收string。表示请求的类型，如"GET"、"HEAD"、"DELETE"等。无默认值
    + url：接收string。表示字符串形式的网址。无默认值
    + fields：接收dict.表示请求类型所带的参数。默认为None
    + headers：接收dict.表示请求头所带参数。默认为None
    + **urlopen_kw：接收dict或其他Python中的类型的数据。依据具体需要及请求的类型可添加的参数，通常参数赋值为字典类型或为具体数据。无默认值

    

### 请求头处理

​	在request方法中，如果需要传入headers参数，可通过定义一个字典类型实现。定义一个包含User-Agent信息的字典，使用浏览器为火狐和chrome浏览器，操作系统为“Windows NT 6.1; Win64; x64”，向网站“http://www.tipdm.com/tipdm/index.html”发送带headers参数的GET请求，hearders参数为定义的User-Agent字典。



### Timeout设置

​	为防止因为网络不稳定、服务器不稳定等问题造成连接不稳定时的丢包，可以在请求中增加timeout参数设置，通常为浮点数。依据不同需求，timeout参数提供多种设置方法，**可直接在URL后设置该次请求的全部timeout参数**，也可分别设置该次请求的连接与读取timeout参数，**在PoolManager实例中设置timeout参数可应用至该实例的全部请求中**。



### 请求重试设置

​	urllib3库可以通过设置retries参数对重试进行控制。默认进行3次请求重试，并进行3次重定向。自定义重试次数通过赋值一个整型给retries参数实现，可通过定义retries实例来定制请求重试次数及重定向次数。若需要同时关闭请求重试及重定向则可以将retries参数赋值为False，仅关闭重定向则将redirect参数赋值为False。与Timeout设置类似，可以在PoolManager实例中设置retries参数控制全部该实例下的请求重试策略。



### 生成完整HTTP请求

​	使用urllib3库实现生成一个完整的请求，该请求应当包含链接、请求头、超时时间和重试次数设置。





## 使用requests库实现

​	requests库是一个原生的HTTP库，比urllib3库更为容易使用。requests库发送原生的HTTP 1.1请求，无需手动为URL添加查询字串，也不需要对POST数据进行表单编码。相对于urllib3库，requests库拥有完全自动化Keep-alive和HTTP连接池的功能。requests库包含的特性如下。

+ 连接特性
    + Keep-Alive&连接池
    + 基本摘要式的身份认证
    + 文件分块上传
    + 国际化域名和URL
    + 优雅的key/value Cookie
    + 流下载
    + 带持久Cookie的会话
    + 自动解压
    + 连接超时
    + 浏览器式的SSL认证
    + Unicode响应体
    + 分块请求
    + 自动内容解码
    + HTTP(S)代理支持
    + 支持.netrc



### 生成请求

+ requests库生成请求的代码非常便利，其使用的request方法的语法格式如下。 

+ requests.request.method**(**url**,****kwargs)
    + 参数——说明
    + method：接收string。表示请求的类型，如"GET"、"HEAD"、"DELETE"等。无默认值
    + url：接收string。表示字符串形式的网址。无默认值
    + **kwargs：接收dict或其他Python中的类型的数据。依据具体需要及请求的类型可添加的参数，通常参数赋值为字典类型或为具体数据



### 查看状态码与编码

+ 需要注意的是，当requests库猜测错时，需要手动指定encoding编码，避免返回的网页内容解析出现乱码。

+ 手动指定的方法并不灵活，无法自适应对应爬取过程中不同网页的编码，而使用chardet库比较简便灵活，chardet库是一个非常优秀的字符串∕文件编码检测模块。

+ chardet库使用detect方法检测给定字符串的编码，detect方法常用的参数及其说明如下。
    + bvte_str：接收string。表示需要检测编码的字符串。无默认值



### 请求头与响应头处理

​	requests库中对请求头的处理与urllib3库类似，也使用headers参数在GET请求中上传参数，参数形式为字典。使用headers属性即可查看服务器返回的响应头，通常响应头返回的结果会与上传的请求参数对应。



### Timeout设置

​	为避免因等待服务器响应造成程序永久失去响应，通常需要给程序设置一个时间作为限制，超过该时间后程序将会自动停止等待。在requests库中通过设置timeout这个参数实现，超过该参数设定的秒数后，程序会停止等待。



### 生成完整HTTP请求

​	使用requests库的request方法向网站“http://www.tipdm.com/tipdm/index.html”发送一个完整的GET请求，该请求包含链接、请求头、响应头、超时时间和状态码，并且编码应正确设置。



# 解析网页

## 使用chrome开发者工具查看网页

​	chrome开发者工具各面板功能如下。

+ 面板——说明
    + 元素面板(Elements)：该面板可查看渲染页面所需的HTML、CSS和DOM (Document Object Model) 对象，并可实时编辑这些元素调试页面渲染效果
    + 控制台面板(Console)：该面板记录各种警告与错误信息，并可作为shell在页面上与JavaScript交互
    + 源代码面板(Sources)：该面板中可以设置断点调试JavaScript
    + 网络面板(Network)：该面板可查看页面请求、下载的资源文件及 优化网页加载性能。还可查看HTTP的请求头、响应内容等
    + 性能面板(Performance)：原旧版chrome中的时间线面板(Timeline) ,该页面展示页面加载时所有事件花费时长的完整分析
    + 内存面板(Memory)：原旧版chrome中的分析面板(Profiles) ，提供比性能面板更详细的分析，如可跟踪内存泄露等
    + 应用面板(Application)：原旧版chrome中的资源面板(Profiles) ,该面板可检查加载的所有资源
    + 安全面板(Security)：该面板可调试当前网页的安全和认证等问题并确保网站上已正确地实现HTTPS
    + 审查面板(Audits)：该面板对当前网页的网络利用情况、网页性能方面进行诊断，并给出优化建议



## 使用正则表达式解析网页

### Python正则表达式

​	正则表达式是一种可以用于模式匹配和替换的工具，可以让用户通过使用一系列的特殊字符构建匹配模式，然后把匹配模式与待比较字符串或文件进行比较，根据比较对象中是否包含匹配模式，执行相应的程序。



### 正则表达式的广义化

常用广义化符号

1、英文句号“.”：能代表除换行符“\n”任意一个字符；

2、字符类“[]”：被包含在中括号内部，任何中括号内的字符都会被匹配；

3、管道“|”：该字符被视为OR操作；



+ 部分有特殊含义的符号
    + \w：数字和字母字符: [0-9a-zA-Z]
    + \W：与\w反义
    + \s：空白字符
    + \S：非空白字符
    + \d：数字: [0-9]
    + \D：非数字: [^0-9]
    + \b：单词的边界
    + \B：非单词边界



+ Python正则表达式里的量化符
    + ?：前面的元素是可选的，并且最多匹配1次
    + *：前面的元素会被匹配0次或多次
    + +：前面的元素会被匹配1次或多次
    + {n}：前面的元素会正好被匹配n次
    + {n,}：前面的元素至少会被匹配n次
    + {n,m}：前面的元素至少匹配n次，至多匹配m次



​	使用正则表达式无法很好的定位特定节点并获取其中的链接和文本内容，而使用Xpath和Beautiful Soup能较为便利的实现这个功能。



## 使用Xpath解析网页

### XPath介绍

​	XML路径语言（XML Path Language），它是一种基于XML的树状结构，在数据结构树中找寻节点，确定XML文档中某部分位置的语言。



### 基本语法

+ 使用Xpath需要从lxml库中导入etree模块，还需使用HTML类对需要匹配的HTML对象进行初始化（XPath只能处理文档的DOM表现形式）。HTML类的基本语法格式如下。

+ lxml.etree.**HTML**(text, parser=None, *, base_url=None)

+ 参数名称——说明
    + text：接收str。表示需要转换为HTML的字符串。无默认值
    + parser：接收str。表示选择的HTML解析器。无默认值
    + base_url：接收str。表示设置文档的原始URL,用于在查找外部实体的相对路径。默认为None

+ 若HTML中的节点没有闭合，etree模块也提供自动补全功能。**调用tostring方法即可输出修正后的HTML代码**，但是**结果为bytes类型**，**需要使用decode方法转成str类型**。

+ Xpath使用类似正则的表达式来匹配HTML文件中的内容，常用匹配表达式如下。
+ 表达式——说明
    + nodename：选取nodename节点的所有子节点
    + /：从当前节点选取直接子节点
    + //：从当前节点选取子孙节点
    + .：选取当前节点
    + ..：选取当前节点的父节点
    + @：选取属性



### 谓语

+ Xpath中的谓语用来查找某个特定的节点或包含某个指定的值的节点，谓语被嵌在路径后的方括号中，如下。

+ 表达式——说明
    + /html/body/div[1]：选取属于body子节点下的第一个div节点
    + /html/body/div[last()]：选取属于body子节点下的最后一个div节点
    + /html/body/div[last()-1]：选取属于body子节点下的倒数第二个div节点
    + /html/body/div[positon( <3]：选取属于body子节点下的下前两个div节点
    + /html/body/div[@id]：选取属于body子节点下的带有id属性的div节点
    + /html/body/div[@id=" content”]：选取属于body子节点下的id属性值为content的div节点
    + /html /body/div[xx> 10.00]：选取属于body子节点下的xx元素值大于10的节点



### 功能函数

+ Xpath中还提供功能函数进行模糊搜索，有时对象仅掌握了其部分特征，当需要模糊搜索该类对象时，可使用功能函数来实现，具体函数如下。

+ 功能函数——示例——说明
    + starts-with：//div[starts-with(@id," co" )]
        + 选取id值以co开头的div节点
    + contains：//div[contains(@id." co”)]
        + 选取id值包含co的div节点
    + and：//div[contains(@id," co” )andcontains(@id," en" )]
        + 选取id值包含co和en的div节点
    + text()：//li[contains(text()," first" )]
        + 选取节点文本包含first的div节点



### 提取header节点下全部标题文本及对应链接

+ 使用text方法可以提取某个单独子节点下的文本，若想提取出定位到的子节点及其子孙节点下的全部文本，则需要使用string方法实现。

+ 使用HTML类将其初始化通过requests库获取的网页，之后使用谓语定位id值以me开头的ul节点，并使用te xt方法获取其所有子孙节点a内的文本内容，使用@选取href属性从而实现提取所有子孙节点a内的链接，最后使用string方法直接获取ul节点及其子孙节点中的所有文本内容。



## 使用Beautiful Soup解析网页

​	Beautiful Soup是一个可以从HTML或XML文件中提取数据的Python库。目前Beautiful Soup 3已经停止开发，大部分的爬虫选择使用Beautiful Soup 4开发。Beautiful Soup不仅支持Python标准库中的HTML解析器，还支持一些第三方的解析器，具体语法如下。

+ 解析器——语法格式

    + Python标准库：BeautifulSoup(markup,"html.parser")
        + 优点：
            + Python的内置 标准库
            + 执行速度适中
            + 文档容错能力强
        + 缺点：
            + Python 2.7.3或3.2.2前的版本中文档容错能力差

    + lxml HTML解析器：BeautifulSoup(markup, "lxm!")
        + 优点：
            + 速度快
            + 文档容错能力强
        + 缺点：
            + 需要安装C语言库
    + Ixml XML解析器：BeautifuSoup(markup, [Ixm-xml")或BeautifulSoup(markup, "xml")
        + 优点：
            + 速度快
            + 唯一支持XML的解析器
        + 缺点：
            + 需要安装C语言库
    + html5lib：BeautifulSoup(markup, "htmi5ib)
        + 优点：
            + 最好的容错性
            + 以浏览器的方式解析文档
            + 生成HTML5格式的文档
        + 缺点：
            + 速度慢
            + 
                不依赖外部扩展



### 创建BeautifulSoup对象

+ 要使用Beautiful Soup库解析网页首先需要创建BeautifulSoup对象，将字符串或HTML文件传入Beautiful Soup库的构造方法可以创建一个BeautifulSoup对象，使用格式如下。
    + BeautifulSoup**("data")   #**通过字符串创
    + BeautifulSoup**(open("index.html"))    #**通过**HTML**文件创建

+ 生成的BeautifulSoup对象可通过prettify方法进行格式化输出，其语法格式如下
    + BeautifulSoup.prettify(self, encoding=None, formatter='minimal')
    + prettify方法常用的参数及其说明如下。
    + 参数——说明
        + encoding：接收string。表示格式化时使用的编码。默认为None
        + formatter：接收string。表示格式化的模式。默认为minimal, 表示按最简化的格式化将字符串处理成有效的HTMLXML



### 对象类型

+ **Tag**对象类型
    + Tag对象为HTML文档中的标签，形如“<title>The Dormouse's story</title>”或“<p class="title"><b>The Dormouse's story</b></p>”等HTML标签再加上其中包含的内容便是Beautiful Soup中的Tag对象。
    + 通过Tag的名称属性可以很方便的在文档树中获取需要的Tag对象，通过该方法只能获取文档树中第一个同名的Tag对象，而通过多次调用可获取某个Tag对象下的分支Tag对象。通过find_all方法可以获取文档树中的全部同名Tag对象。
    + Tag有两个非常重要的属性：name和attributes。name属性可通过name方法来获取和修改，修改过后的name属性将会应用至BeautifulSoup对象生成的HTML文档。

+ **NavigableString**对象类型
    + NavigableString对象为包含在Tag中的文本字符串内容，如“<title>The Dormouse‘s story</title>”中的“The Dormouse’s story”，使用string的方法获取，NavigableString对象无法被编辑，但可以使用replace_with的方法进行替换。
+ **BeautifulSoup**对象类型
    + BeautifulSoup对象表示的是一个文档的全部内容。大部分时候，可以把它当作Tag对象。 BeautifulSoup对象并不是真正的HTML或XML的tag，所以并没有tag的name和attribute属性，但其包含了一个值为“[document]”的特殊属性name。
+ **Comment**对象类型
    + Tag、NavigableString、BeautifulSoup几乎覆盖了html和xml中的所有内容，但是还有一些特殊对象，文档的注释部分是最容易与Tag中的文本字符串混淆的部分。Beautiful Soup库中将文档的注释部分识别为Comment类型，Comment对象是一个特殊类型的NavigableString对象，但是当其出现在HTML文档中时，Comment对象会使用特殊的格式输出，需调用prettify方法。



### 搜索特定节点并获取其中的链接及文本

+ Beautiful Soup定义了很多搜索方法，其中常用的有find方法和find_all方法，两者的参数一致，区别为find_all方法的返回结果是值包含一个元素的列表，而find直接返回的是结果。find_all方法用于搜索文档树中的Tag非常方便，其语法格式如下。

+ BeautifulSoup.find_all**(**name,attrs,recursive,string,**kwargs)
+ find_all方法的常用参数及其说明如下。

+ 参数——说明

    + name：接收string。表示查找所有名字为name的tag,字符串对象会被自动忽略掉，搜索name参数的值可以使用任一-类型的过滤器: 字符串、正则表达式、列表、方法或True。 无默认值
    + attrs：接收string。表示查找符合CSS类名的tag,使用class做参数会导致语法错误，从Beautiful Soup的4.1.1版本开始，可以通过class_ 参数搜索有指定CSS类名的tag。无默认值
    + recursive：接收Built-in。表示是否检索当前tag的所有子孙节点。默认为True,若只想搜索tag的直接子节点可将该参数设为False
    + string：接收string。表示搜索文档中匹配传入的字符串的内容，与name参数的可选值一样，string参数也接收多种过滤器。无默认值
    + **kwargs：若一个指定名字的参数不是搜索内置的参数名，搜索时会把该参数当作指定名字tag的属性来搜索

+ find_all方法可通过多种参数遍历搜索文档树中符合条件的所有子节点。

    + 可通过name参数搜索同名的全部子节点，并接收多种过滤器。

    + 按照CSS类名可模糊匹配或完全匹配。完全匹配class的值时，如果CSS类名的顺序与实际不符，将搜索不到结果。
    + 若tag的class属性是多值属性，可以分别搜索tag中的每个CSS类名。
    + 通过字符串内容进行搜索符合条件的全部子节点，可通过过滤器操作。
    + 通过传入关键字参数，搜索匹配关键字的子节点。



# 数据存储

## 将数据存储为JSON文件

+ 将数据存储为JSON文件的过程为一个编码过程，编码过程常用dump函数, 将Python对象转换为JSON对象，并通过fp文件流将JSON对象写入文件内。

+ **json.dump**(**obj**,**fp**,skipkeys=False,**ensure_ascii=True**,check_circular=True,allow_nan=True,cls=None,indent=None,separators=None,**encoding='utf-8'**, default=None**,sort_keys=False**, **kw)

+ dump函数常用参数及其说明如下。

+ 参数——说明
    + skipkeys：接收Built-in。表示是否跳过非Python基本类型的key,若dict的keys内的数据为非Python基本类型，即不是str、unicode、 int、 long、 float、bool、 None等类型，设置该参数为False时，会报TypeError错误。默认值为False, 设置为True时，跳过此类key
    + ensure_ascii：接收Built-in。表示显示格式，若dict内含有非ASCII的字符，则会以类似"\uXXX" 的格式显示默认值为True,设置为False后，将会正常显示
    + indent：接收int。表示显示的行数，若为0或为None,则在一行内显示数据， 否则将 会换行且按照indent的数量显示前面的空白，将JSON内容格式化显示。默认为None
    + separator：接收string。表示分隔符，实际上为(item_separator,dict _separator) 的一个元组，默认为(',', ':')，表示dictionary内的keys之间用","隔开，而key和value之间":"隔开。默认为None
    + encoding：接收string。表示设置的JSON数据的编码形式，处理中文时需要注意此参数的值。默认为UTF-8
    + sort_keys：接收Built-in。表示是否根据keys的值进行排序。默认为False,为True时数据将根据keys的值进行排序



## 将数据存储入MySQL数据库

+ pandas提供了读取与存储关系型数据库数据的函数与方法。除了pandas库外，还需要使用SQLAlchemy库建立对应的数据库连接。SQLAlchemy配合相应数据库的Python连接工具（例如MySQL数据库需要安装mysqlclient或者pymysql库），使用create_engine函数，建立一个数据库连接。

+ creat_engine中填入的是一个连接字符串。在使用Python的SQLAlchemy时，MySQL和Oracle数据库连接字符串的格式如下

+ *数据库产品名**+**连接工具名：**//**用户名**:**密码**@**数据库**IP**地址**:**数据库端口号**/**数据库名称？charset =* *数据库数据编码*

+ 数据存储用pandas的to_sql方法。
    + DataFrame.**to_sql**(name, con, schema=None, if_exists=’fail’, index=True, index_label=None, *dtype=None)*
    + 参数名称——说明
        + name：接收string。代表数据库表名。无默认。
        + con：接收数据库连接。无默认。
        + if_exists接收fail, replace, append. fai表示如果表名存在则不执行写入操作; replace表示如果存在，将原数据库表删除，再重新创建; append则表示在原数据库表的基础上追加数据。默认为fail。
        + index：接收boolean。表示是否将行索引作为数据传入数据库。默认True。
        + index_label：接收string或者sequence.代表是否引用索引名称，如果index参数为True此参数为None则使用默认名称。如果为多重索引必须使用sequence形式。默认为None
        + dtype：接收dict. 代表写入的数据类型(列名为key, 数据格式为values) 。默认为None



# 逆向分析爬取动态网页

就是通过开发者工具查看JS动态生成的文件，从而提取出数据







# 使用Selenium库爬取动态网页

## 安装Selenium库以及下载浏览器补丁

+ 以Chrome浏览器的chromedrive补丁为例，在安装好Selenium 3.9.0之后，下载并安装chromedrive补丁的步骤如下。

+ 在Selenium官网下载对应版本的补丁。根据操作系统选择chromedrive文件。

+ 将下载好的chromedrive.exe文件，存放至python安装根目录（与python.exe文件同一目录）即可。



## 页面等待

+ Selenium Webdriver提供两种类型的等待——**隐式和显式**。
    + 显式的等待使网络驱动程序在继续执行之前等待某个条件的发生。
    + 隐式的等待使WebDriver在尝试定位一个元素时，在一定的时间内轮询DOM。
    + 在爬取“http://www.ptpress.com.cn/shopping/index”网页搜索“Python编程”关键词过程中，用到了显示等待，
    + 主要介绍显示等待。显式等待是指定某个条件，然后设置最长等待时间。如果在这个时间还没有找到元素，那么便会抛出异常，在登录“http://www.ptpress.com.cn/shopping/index”网页等待10秒。

+ **WebDriverWait函数是默认每500毫秒调用一次ExpectedCondition**，直到成功返回。ExpectedCondition的**成功返回类型是布尔值**，对于所有其他ExpectedCondition类型，则返回True或非Null返回值。如果在10秒内不能发现元素返回，就会在抛出TimeoutException异常。

+ WebDriverWait的语法使用格式如下。
    + WebDriverWait(driver,等待时间)



## 预期的条件

​	在自动化Web浏览器时，不需要手动编写期望的条件类，也不必为自动化创建实用程序包，Selenium库提供了一些便利的判断方法如表 4‑2所示，在爬取“http://www.ptpress.com.cn/search/books”网页搜索“Python编程”关键词的过程中，用到了element_to_be_clickable方法、元素是否可点击等判断方法。

+ 方法——作用
    + title_is：标题是某内容
    + title_contains：标题包含某内容
    + presence_of_element_located：元素加载出，传入定位元组，如(By.ID, 'p')
    + visibility_of_element_located：元素可见，传入定位元组
    + visibility_of：传入元素对象
    + presence_of_all_elements_located：所有元素加载出
    + text_to_be_present_in_element：某个元素文本包含某文字
    + text_to_be_present_in_element_value：某个元素值包含某文字
    + frame_to_be_available_and_switch_to_it_frame：加载并切换
    + invisbility_of_element_located：元素不可见
    + element_to_be_clickable：元素可点击
    + staleness_of：判断一个元素是否仍在DOM，可判断页面是否已经刷新
    + element_to_be_selected：元素可选择，传元素对象
    + element_located_to_be_selected：元素可选择，传入定位元组
    + element_selection_state_to_be：传入元素对象以及状态，相等返回True,否则返回False 
    + element_located_selection_state_to_be：传入定位元组以及状态，相等返回True,否则返回False
    + alert_is_present：是否出现Alert



## 元素选取

​	在页面中定位元素有多种策略。Selenium库提供了如表所示的方法来定位页面中的元素，使用find_element进行元素选取。在单元素查找中使用到了通过元素ID进行定位、通过XPath表达式进行定位、通过CSS选择器进行定位等操作。在多元素查找中使用到了通过CSS选择器进行定位等操作。

+ 定位一个元素——定位多个元素——含义
+ find_element_by_id
    + find_elements_by_id
        + 通过元素ID进行定位
+ find_element_by_name
    + find_elements_by_name
        + 通过元素名称进行定位
+ find_element_by_xpath
    + find_elements_by_xpath
        + 通过XPath表达式进行定位
+ find_element_by_link_text
    + find_elements_by_link_text
        + 通过完整超链接文本进行定位
+ find_element_by_partial_link_text
    + find_elements_by_partial_link_text
        + 通过部分超链接文本进行定位
+ find_element_by_tag_name
    + find_elements_by_tag_name
        + 通过标记名称进行定位
+ find_element_by_class_name
    + find_elements_by_class_name
        + 通过类名进行定位
+ find_element_by_css_selector
    + find_elements_by_css_selector
        + 通过CSS选择器进行定位



+ Selenium库的find_element的语法使用格式如下。

+ driver.find_element_by_id(By.method,selector_url)



## 页面操作

### 填充表单

​	HTML表单包含了表单元素，而表单元素指的是不同类型的input元素、复选框、单选按钮、提交按钮等。填写完表单后，需要提交表单。





# 使用表单登录方法实现模拟登录

## 查找需要提交的表单数据

​	使用Chrome开发者工具获取到的提交入口，观察Headers标签，Form Data信息为服务器端接收到的表单数据，如图 5-4所示。其中，username表示账号；password表示密码；captcha表示验证码；returnUrl表示跳转网址。returnUrl系自动生成，在登录网页时无需输入。



### 处理验证码

+ 识别验证码。人工识别验证码分为3个步骤：①获取生成验证码的图片地址；②将验证码图片下载到本地；③人工识别验证码。

   获取登录网页验证码地址的步骤如下。

+ 打开网站，进入登录网页，若已登录需先退出。打开Chrome开发者工具后打开网络面板，按“F5”键刷新网页。

+ 观察Chrome开发者工具左侧的资源，找到“captcha.svl”资源并单击，观察右侧的Preview标签，若显示验证码图片如左图所示，则“captcha.svl”资源的Request URL信息为验证码图片的地址，（不同网站的验证码名字不同）

+ 获取验证码图片地址后，下一步对图片地址发送请求，将图片下载到本地，最后人工打开图片识别验证码。

+ 代理IP跳过验证码。很多时候在登录后爬取过程中也会弹出验证码，当使用同一个IP长时间高频率爬取网页时，该网站的服务器可能会判定该IP在爬取数据，触发网站的安全机制，在客户端弹出验证码，只有输入验证码后，客户端的访问请求才能继续被接受和处理。

+ 每次输入验证码会比较麻烦，效率低下。而且当网站服务器多次对指定IP弹出验证码后，可能会封禁该IP，导致爬取无法进行。因此，使用代理IP的方法，使用多个IP切换跳过验证码，成为应对反爬虫的主要手段。

    + 获取代理IP

    a.VPN：是Virtual Private Network的简称，指专用虚拟网络。国内外很多厂商都提供VPN服务，可自动更换IP，实时性高，速度快，但价格较高，适合商用。

    b.IP代理池：指大量IP地址集。国内外很多厂商将IP做成代理池，提供API接口，允许用户使用程序调用，但价格也较高。

    c.ADSL宽带拨号：是一种宽带上网方式。特点是断开重连会更换IP，爬虫使用这个原理更换IP，但效率低，实时性差。

+ 使用Requests库配置代理IP

     Requests库，为各个发送请求的函数（get、post、put等）配置代理IP的参数是proxies，它接收dict。为保障安全性，一些代理服务器设置了用户名和密码，使用它的IP时需要带上用户名和密码，IP地址的基本格式如下。

    http://**用户名**:**密码**@服务器地址



## 使用POST请求方法登录

+ POST请求方法能够保障用户端提交数据的安全性，因此它被一般需要登录的网站采用。Requests库的post函数能够以POST请求方法向服务器端发送请求，它返回一个Response <Response>对象。post函数的基本语法格式如下。

+ requests.post(url, data=None, json=None, **kwargs)
    + url：接收string。表示提交入口。无默认值
    + data：接收dict。表示需要提交的表单数据。无默认值
+ Cookie用于服务器端识别客户端，当发送请求的客户端享有同样的Cookie时，即可认定客户端是同一个。
+ Requests库的会话对象Session能够跨请求地保持某些参数，比如Cookie，它令发送请求的客户端享有相同的Cookie，保证表单数据的匹配。以POST请求方法为例，通过Session发送请求的基本语法格式如下。
    + s = requests.Session()
    + s.post**(**url, data=None, json=None, ****kwargs)



# 使用Cookie登录方法实现模拟登录

## 使用浏览器Cookie登录

### 获取Cookie

获取Cookie分以下两步进行。

+ 登录网站。输入账号、密码、验证码，保证成功登录网站。

+ 找到登录成功后返回的页面地址的Cookie。步骤如下。

+ 打开Chrome开发者工具后打开网络面板，按“F5”键刷新网页，出现资源。找到左侧的“index.html”资源（不同页面不一样），它代表的是本网页地址，可以看到Request URL信息与本网页地址相吻合

+ 观察右侧Headers标签，查看Request Headers信息，找到Cookie信息，它即为登录成功后的Cookie，将其保存下来



### 携带Cookie发送请求

+ 设置发送请求时携带Cookie的参数是cookies，它接收dict或CookieJar。从浏览器获取的Cookie为str类型，需要处理成dict。以GET请求方法为例，携带Cookie发送请求的基本语法格式如下。

+ requests.get(url, cookies, **kwargs)





## 基于表单登录的Cookie登录

### 存储Cookie

+ 存储和加载Cookie需要用到http库的cookiejar模块，它提供可存储Cookie的对象。cookiejar模块下的FileCookieJar用于将Cookie保存到本地磁盘和从本地磁盘加载Cookie文件，LWPCookieJar是FileCookieJar的子类。LWPCookieJar对象存储和加载的Cookie文件格式为Set-Cookie3，是比较常用的一种，也是本小节需要用的。创建LWPCookieJar对象的函数是LWPCookieJar，其基本语法格式如下。

+ http.cookiejar.LWPCookieJar(filename,delayload = None)
    + filename：接收string，表示需要打开或保存的文件名，无默认值

+ LWPCookieJar对象的save方法用于保存Cookie，其基本语法格式如下。
    + *http.cookiejar.LWPCookieJar.save(filename=None,* ignore_discard=False,ignore_expires=False)
    + 参数——说明
    + filename：接收string。需要保存的文件名。默认为空
    + ignore_discard：接收bool。表示即使Cookie将被丢弃也将它保存下来。默认为False
    + ignore_ expires：接收bool。表示如果在该文件中Cookie已经存在，则覆盖原文件写入。默认为False



### 加载Cookie

+ LWPCookieJar对象的load方法用于加载Cookie，其基本语法格式如下。

+ http.cookiejar.LWPCookieJar.load(filename=None,ignore_discard=False,ignore_expires=False)
+ 说明——参数
    + filename：接受string。表示需要动加教的Cookie文件名。默认为None
    + ignore_discard：接受bool，表示Cookie不存在也加载。默认为False
    + ignore_expires：接受bool，表示疆盖原有Cookie默认为False