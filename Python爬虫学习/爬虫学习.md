# 认识爬虫与反爬虫

## 浏览网页基本流程：

+ 浏览器**发送请求**给服务器
+ 服务器**返回响应**内容给浏览器



## 爬虫：

网络爬虫也被称为网络蜘蛛、网络机器人，是一个自动下载网页的计算机程序或自动化脚本。

网络爬虫就像一只蜘蛛一样在互联网上沿着URL的丝线爬行，下载每一个URL所指向的网页，分析页面内容。



### 通用网络爬虫

​	通用网络爬虫又称为全网爬虫，其爬行对象由一批种子URL扩充至整个Web，该类爬虫比较适合为搜索引擎搜索广泛的主题，主要由搜索引擎或大型Web服务提供商使用。

+ **深度优先策略：**按照深度由低到高的顺序，依次访问下一级网页链接，直到无法再深入为止。

+ **广度优先策略：**按照网页内容目录层次的深浅来爬行，优先爬取较浅层次的页面。当同一层中的页面全部爬行完毕后，爬虫再深入下一层。



### 聚焦网络爬虫

​	聚焦网络爬虫又被称作主题网络爬虫，其最大的特点是只选择性地爬行与预设的主题相关的页面。

+  **基于内容评价的爬行策略：**该种策略将用户输入的查询词作为主题，包含查询词的页面被视为与主题相关的页面。

+ **基于链接结构评价的爬行策略：**该种策略将包含很多结构信息的半结构化文档Web页面用来评价链接的重要性，其中一种广泛使用的算法为PageRank算法。

+ **基于增强学习的爬行策略：**该种策略将增强学习引入聚焦爬虫，利用贝叶斯分类器对超链接进行分类，计算出每个链接的重要性，按照重要性决定链接的访问顺序。

+ **基于语境图的爬行策略：**该种策略通过建立语境图学习网页之间的相关度，计算当前页面到相关页面的距离，距离越近的页面中的链接优先访问。



### 增量式网络爬虫

​	增量式网络爬虫只对已下载网页采取增量式更新或只爬行新产生的及已经发生变化的网页，需要通过重新访问网页对本地页面进行更新，从而保持本地集中存储的页面为最新页面。

​	常用的更新方法如下。

+ **统一更新法：**以相同的频率访问所有网页，不受网页本身的改变频率的影响。

+ **个体更新法：**根据个体网页的改变频率来决定重新访问各页面的频率。

+ **基于分类的更新法：**爬虫按照网页变化频率分为更新较快和更新较慢的网页类别，分别设定不同的频率来访问这两类网页。



### 深层网络爬虫

​	Web页面按照存在方式可以分为表层页面和深层页面两类。表层页面指以传统搜索引擎可以索引到的页面，深层页面为大部分内容无法通过静态链接获取，隐藏在搜索表单后的，需要用户提交关键词后才能获得的Web页面。

​	深层爬虫的核心部分为表单填写，包含以下两种类型。

+ **基于领域知识的表单填写：**该种方法一般会维持一个本体库，通过语义分析来选取合适的关键词填写表单。

+ **基于网页结构分析的表单填写：**这种方法一般无领域知识或仅有有限的领域知识，将HTML网页表示为DOM树形式，将表单区分为单属性表单和多属性表单，分别进行处理，从中提取表单各字段值。



## 爬虫的合法性与robots协议

### 爬虫的合法性

​	目前，多数网站允许将爬虫爬取的数据用于个人使用或者科学研究。但如果将爬取的数据用于其他用途，尤其是转载或者商业用途，严重的将会触犯法律或者引起民事纠纷。

​	以下两种数据是不能爬取的，更不能用于商业用途。

+ **个人隐私数据：**如姓名、手机号码、年龄、血型、婚姻情况等，爬取此类数据将会触犯个人信息保护法。

+ **明确禁止他人访问的数据：**例如用户设置了账号密码等权限控制，进行了加密的内容。

    

    还需注意版权相关问题，有作者署名的受版权保护的内容不允许爬取后随意转载或用于商业用途。



### robots协议

+ robots协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（ Robots Exclusion Protocol ），当使用一个爬虫爬取一个网站的数据时，需要遵守网站所有者针对所有爬虫所制定的协议。

+ 它通常是一个叫作robots.txt的文本文件，该协议通常存放在网站根目录下，里面规定了此网站哪些内容可以被爬虫获取，及哪些网页是不允许爬虫获取的。

+ robots.txt 的样例

    + ```
        User-agent: * 
        Disallow: /
        Allow: /public/
        ```

        + 这实现了对所有搜索爬虫只允许爬取public目录的功能，将上述内容保存成robots.txt文件，放在网站的根目录下，和网站的入口文件（比如index.php、index.html和index.jsp等）放在一起。 
        + 上面的User-agent描述了搜索爬虫的名称，这里将其设置为＊则代表该协议对任何爬取爬虫有效。 比如，我们可以设置：User-agent: Baiduspider 。这就代表我们设置的规则对百度爬虫是有效的。 如果有多条User-agent记录，则就会有多个爬虫会受到爬取限制，但至少需要指定一条。 Disallow 指定了不允许抓取的目录，比如上例子中设置为／则代表不允许抓取所有页面。Allow一般和Disallow一起使用，一般不会单独使用，用来排除某些限制。 现在我们设置为/public／，则表示所有页面不允许抓取，但可以抓取public目录。

    + 禁止所有爬虫访问任何目录的代码如下:

    + ```
        User-agent: * 
        Disallow: /
        ```

    + 允许所有爬虫访问任何目录的代码如下：

    + ```
        User-agent: * 
        Disallow: 
        ```

    + 禁止所有爬虫访问网站某些目录的代码如下：

    + ```
        User-agent: * 
        Disallow: /private/ 
        Disallow: /tmp/
        ```

    + 只允许某一个爬虫访问的代码如下：

    + ```
        User-agent: WebCrawler
        Disallow: 
        User-agent: * 
        Disallow: /
        ```



### robotparser模块

+ 了解 Robots 协议之后，我们就可以使用robotparser模块来解析robots.txt了。 该模块提供了一个类RobotFileParser，它可以根据某网站的robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。 

+ 该类用起来非常简单，只需要在构造方法里传入robots.txt的链接即可。 首先看一下它的声明：urllib.robotparser.RobotFileParser(url=’’)，当然也可以在声明时不传入，默认为空，最后再使用 set_url（）方法设置一下也可。 
+ 下面列举了这个类常用的几个方法。 
    + set_url（）：用来设置robots.txt文件的链接。 如果在创建RobotFileParser对象时传入了链接，那么就不需要再使用这个方法设置了。 
    + read（）：读取robots.txt文件并进行分析。 注意，这个方法执行一个读取和分析操作，如果不调用这个方法， 接下来的判断都会为False，所以一定记得调用这个方法。 这个方法不会返 回任何内容，但是执行了读取操作。 
    +  parse（）：用来解析robots.txt文件，传人的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。 
    + can_fetch（）：该方法传入两个参数， 第一个是User-agent，第二个是要抓取的URL。 返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是True或False。
    + mtime（）：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的robots.txt。 
    + modified（） ：它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取 和分析robots.txt的时间。



## 网站反爬虫的目的与手段

### 通过User-Agent校验反爬

​	浏览器在发送请求的时候，会附带一部分浏览器及当前系统环境的参数给服务器，服务器会通过User-Agent的值来区分不同的浏览器。



### 通过访问频度反爬

+ 普通用户通过浏览器访问网站的速度相对爬虫而言要慢的多，所以不少网站会利用这一点对访问频度设定一个阈值，如果一个IP单位时间内访问频度超过了预设的阈值，将会对该IP做出访问限制。

+ 通常需要经过验证码验证后才能继续正常访问，严重的甚至会禁止该IP访问网站一段时间。



### 通过验证码校验反爬

​	有部分网站不论访问频度如何，一定要来访者输入验证码才能继续操作。例如12306网站，不管是登陆还是购票，全部需要验证验证码，与访问频度无关。



### 通过变换网页结构反爬

​	一些社交网站常常会更换网页结构，而爬虫大部分情况下都需要通过网页结构来解析需要的数据，所以这种做法也能起到反爬虫的作用。在网页结构变换后，爬虫往往无法在原本的网页位置找到原本需要的内容。



### 通过账号权限反爬

+ 部分网站需要登录才能继续操作，这部分网站虽然并不是为了反爬虫才要求登录操作，但确实起到了反爬虫的作用。

+ 例如微博查看评论就需要登录账号。



## 爬取策略制定

​	针对之前介绍的常见的反爬虫手段，可以制定对应的爬取策略如下。

+ **发送模拟User-Agent**：通过发送模拟User-Agent来通过检验，将要发送至网站服务器的请求的User-Agent值伪装成一般用户登录网站时使用的User-Agent值。

+ **调整访问频度：**通过备用IP测试网站的访问频率阈值，然后设置访问频率比阈值略低。这种方法既能保证爬取的稳定性，又能使效率又不至于过于低下。

+ **通过验证码校验：**使用IP代理，更换爬虫IP；通过算法识别验证码；使用cookie绕过验证码。

+ **应对网站结构变化：**只爬取一次时，在其网站结构调整之前，将需要的数据全部爬取下来；使用脚本对网站结构进行监测，结构变化时，发出告警并及时停止爬虫。

+ **通过账号权限限制：**通过模拟登录的方法进行规避，往往也需要通过验证码检验。

+ **通过代理IP规避**：通过代理进行IP更换可有效规避网站检测，需注意公用IP代理池是 网站重点监测对象。



## 可抓取的数据

+ 网页文本：如HTML文档、Json格式文本等。

+ 图片：获取到的是二进制文件，保存为图片格式。

+ 视频：同为二进制文件，保存为视频格式即可。

+ 其他：只要是能请求到的，都能获取。





# Python爬虫相关库介绍与配置

## Python爬虫相关库

​	目前Python有着形形色色的爬虫相关库，按照库的功能，整理如下。

+ 类型——库名——简介
    + **通用**
        + urllib：Python内置的HTTP请求库，提供一系列用于操作URL的功能
        + requests：基于urllib,采用Apache2 Licensed开源协议的HTTP库
        + urllib 3：提供很多Python标准库里所没有的重要特性:线程安全，连接池，客户端SSL/TLS验证，文件分部编码上传，协助处理重复请求和HTTP重定位，支持压缩编码，支持HTTP和SOCKS代理，100%测试覆盖率
    + **框架**
        + scrapy：一个为了爬取网站数据，提取结构性数据而编写的应用框架
    + **HTML/XML**解析器
        + lxml：C语言编写高效HTML/XML处理库。支持XPath解析器
        + BeautifulSoup 4：纯Python实现的HTML/XML处理库， 效率相对较低





# 认识HTTP协议

## HTTP请求方式与过程

爬虫在爬取数据时将会作为客户端模拟整个HTTP通信过程，该过程也需要通过HTTP协议实现。HTTP请求过程如下。

+ 由HTTP客户端向服务器发起一个请求，创建一个到服务器指定端口（默认是80端口）的TCP连接。

+ HTTP服务器从该端口监听客户端的请求。

+ 一旦收到请求，服务器会向客户端返回一个状态，比如“HTTP/1.1 200 OK”，以及返回的响应内容，如请求的文件、错误消息、或其它信息。



### 请求方法

+ 在HTTP/1.1协议中共定义了8种方法（也叫“动作”）来以不同方式操作指定的资源，常用方法有GET、HEAD、POST等。

+ 请求方法——方法描述
    + GET：请求指定的页面信息，并返回实体主体。GET可能会被网络爬虫等随意访问，因此GET方法应该只用在读取数据，而不应当被用于产生“副作用”的操作中，例如在Web Application中
    + HEAD：与GET方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回具体的内容，使用这个方法可以在不必传输全部内容的情况下，就可以获取其中该资源的相关信息(元信息或称元数据
    + POST：向指定资源提交数据，请求服务器进行处理(例如提交表单或者上传文件)。数据会被包含在请求中，这个请求可能会创建新的资源或修改现有资源，或二者皆有
    + PUT：从客户端上传指定资源的最新内容，即更新服务器端的指定资源。



### 请求（request）与响应（response）

HTTP协议采用了请求／响应模型。

+ 客户端向服务器发送一个请求报文，请求报文包含请求的方法、URL、协议版本、请求头部和请求数据。

+ 服务器以一个状态行作为响应，响应的内容包括协议的版本、响应状态、服务器信息、响应头部和响应数据。

+ 客户端与服务器间的请求与响应的具体步骤如下。
    + **连接Web服务器**：由一个HTTP客户端发起连接，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接。
    + **发送HTTP请求**：客户端经TCP套接字向Web服务器发送一个文本的请求报文。
    + **服务器接受请求并返回HTTP响应**：Web服务器解析请求，定位该次的请求资源。之后将资源复本写至TCP套接字，由客户端进行读取。
    + **释放连接TCP连接**：若连接的connection模式为close，则由服务器主动关闭TCP连接，客户端将被动关闭连接，释放TCP连接；若connection模式为keepalive，则该连接会保持一段时间。
    + **客户端解析HTML内容**：客户端首先会对状态行进行解析，之后解析每一个响应头，最后读取响应数据。



### Request

+ 请求方式：主要有GET、POST两种类型，另外还有HEAD、PUT、DELETE、OPTIONS等。

+ 请求URL：URL全称统一资源定位符，如一个网页文档、一张图片、一个视频等都可以用URL唯一确定。

+ 请求头：包含请求时的头部信息，如User-Agent、Host、Cookies等信息。

+ 请求体：请求时额外携带的数据，如表单提交时的表单数据



### Response

+ 响应状态：有多种响应状态，如200代表成功、301跳转、404找不到页面、502服务器错误

+ 响应头：如内容类型、内容长度、服务器信息、设置Cookie等等。

+ 响应体：最主要的部分，包含了请求资源的内容，如网页HTML、图片二进制数据等。



## 常见HTTP状态码

### HTTP状态码种类

​	HTTP状态码是用来表示网页服务器响应状态的3位数字代码，按首位数字分为5类状态码。

+ 状态码类型——状态码意义
    + 1XX：表示请求已被接受，需接后续处理。这类响应是临时响应，只包含状态行和某些可选的响应头信息，并以空行结束
    + 2XX：表示请求已成功被服务器接收、理解并接受
    + 3XX：表示需要客户端采取进一-步的操作才 能完成请求。通常用来重定向，重定向目标需在本次响应中指明
    + 4XX：表示客户端可能发生了错误，妨碍了服务器的处理。
    + 5XX：表示服务器在处理请求的过程中有错误或者异常状态发生，也有可能是服务器以当前的软硬件资源无法完成对请求的处理。



### 常见HTTP状态码

​	HTTP状态码共有67种状态码，常见的状态码如下。

+ 常见状态码——状态码含义
    + **200 OK**：请求成功，请求所希望的响应头或数据体将随此响应返回。
    + **400 Bad Request**：由于客户端的语法错误、无效的请求或欺骗性路由请求，服务器不会处理该请求
    + **403 Forbidden**：服务器已经理解该请求，但是拒绝执行，将在返回的实体内描述拒绝的原因，也可以不描述仅返回404响应
    + **404 Not Found**：请求失败，请求所希望得到的资源未被在服务器上发现，但允许用户的后续请求
    + **500 Internal Server Error**：通用错误消息，服务器遇到了一一个未曾预料的状况，导致了它无法完成对请求的处理，不会给出具体错误信息
    + **503 Service Unavailable**：由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是暂时的，并且将在一段时间以后恢复



## HTTP头部信息

​	HTTP头部信息（HTTP header fields）是指在超文本传输协议（HTTP）的请求和响应消息中的消息头部分。头部信息定义了一个超文本传输协议事务中的操作参数。在爬虫中需要使用头部信息向服务器发送模拟信息，通过发送模拟的头部信息将自己伪装成一般的客户端。



### HTTP头部类型

​	HTTP头部类型按用途可分为：通用头，请求头，响应头，实体头。

+ **通用头：**既适用于客户端的请求头，也适用于服务端的响应头。与HTTP消息体内最终传输的数据是无关的，只适用于要发送的消息。

+ **请求头：**提供更为精确的描述信息，其对象为所请求的资源或请求本身。新版HTTP增加的请求头不能在更低版本的HTTP中使用，但服务器和客户端若都能对相关头进行处理，则可以在请求中使用。

+ **响应头：**为响应消息提供了更多信息。例如，关于资源位置的描述Location字段，以及关于服务器本身的描述使用Server字段等。与请求头类似，新版增加的响应头也不能在更低版本的HTTP版本中使用。

+ **实体头：**提供了关于消息体的描述。如消息体的长度Content-Length，消息体的MIME类型Content-Type。新版的实体头可以在更低版本的HTTP版本中使用。



+ 字段名——说明——示例
    + Accept：可接受的响应内容类型(Content-Types)
        + Accept: text/plain 
    + Accept-Charset：可接受的字符集
        + Accept-Charset:utf-8
    + Accept-Encoding：可接受的响应内容的编码方式
        + Accept- Encoding:gzip,deflate
    + Accept-Language：可接受的响应内容语言列表
        + Accept-Language:en-US
    + Cookie：由之前服务器通过Set-Cookie设置的一个HTTP协议Cookie
        + Cookie:$Version= 1;Skin=new;
    + Referer：设置前一个页面的地址，并且前一个页面中的连接指向当前请求，意思就是如果当前请求是在A页面中发送的，那么referer就是A页面的ur!地址
        + Referer:http://zh.wikipedia.org/wiki/Main_Page
    + User-Agent：用户代理的字符串值
        + User-Agent:Mozilla/5.0(X11;Linuxx86_ 64;rv:12.0)Gecko/201 00101 Firefox/21.0



## 熟悉Cookie

+ HTTP是一种无状态的协议，客户端与服务器建立连接并传输数据，在数据传输完成后，本次的连接将会关闭，并不会留存相关记录。

+ 服务器无法依据连接来跟踪会话，也无法从连接上知晓用户的历史操作。这严重阻碍了基于Web应用程序的交互，也影响用户的交互体验。

+ 某些网站需要用户登录才进一步操作，用户在输入账号密码登录后，才能浏览页面。对于服务器而言，由于HTTP的无状态性，服务器并不知道用户有没有登录过，当用户退出当前页面访问其他页面时，又需重新再次输入账号及密码。



### Cookie机制

​	为解决HTTP的无状态性带来的负面作用，Cookie机制应运而生。Cookie本质上是一段文本信息。

+ 当客户端请求服务器时，若服务器需要记录用户状态，就在响应用户请求时发送一段Cookie信息。

+ 客户端浏览器会保存该Cookie信息，当用户再次访问该网站时，浏览器会把Cookie做为请求信息的一部分提交给服务器。

+ 服务器对Cookie进行验证，以此来判断用户状态，当且仅当该Cookie合法且未过期时，用户才可直接登录网站。



### Cookie的存储方式

​	Cookie由用户客户端浏览器进行保存，按其存储位置可分为内存式存储和硬盘式存储。

+ 内存式存储将Cookie保存在内存中，在浏览器关闭后就会消失，由于其存储时间较短，因此也被称为非持久Cookie或会话Cookie。

+ 硬盘式存储将Cookie保存在硬盘中，其不会随浏览器的关闭而消失，除非用户手工清理或到了过期时间。由于硬盘式Cookie存储时间是长期的，因此也被称为持久Cookie。



### Cookie的实现过程

​	客户端与服务器间的Cookie实现过程的具体步骤如下。

+ **客户端请求服务器：**客户端请求网站页面

+ **服务器响应请求：**Cookie是一种字符串，为key=value形式，服务器需要记录这个客户端请求的状态，在响应头中增加一个Set-Cookie字段。

+ **客户端再次请求服务器：**客户端会对服务器响应的Set-Cookie头信息进行存储。当再次请求时，将会在请求头中包含服务器响应的Cookie信息。



