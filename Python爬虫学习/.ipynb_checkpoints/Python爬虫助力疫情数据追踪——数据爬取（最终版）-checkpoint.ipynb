{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 国内数据存储路径\n",
    "filepath_cn = \"./疫情数据/国内数据\"\n",
    "if not os.path.exists(filepath_cn): # 先判断这个文件夹是否已存在，已存在说明数据时最新的\n",
    "    os.mkdir(filepath_cn) # 创建用最后更新时间来创建文件夹\n",
    "filepath_fg = \"./疫情数据/外国数据\"\n",
    "\n",
    "# 定义一个存储DataFrame的函数，包装了to_csv()方法\n",
    "def df_to_csv(df=None, filepath=None, filename=None, encoding='GBK', retain_index=False):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        df：DataFrame数据\n",
    "        filepath：文件存储路径\n",
    "        filename：文件名\n",
    "        encoding：文件存储编码格式\n",
    "        retain_index：是否保留DataFrame的索引\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath): # 先判断这个文件夹是否已存在，已存在说明数据时最新的\n",
    "        os.mkdir(filepath) # 创建时，文件夹名里有时间来区分数据是否为最新\n",
    "    df.to_csv(filepath + '/' + filename, encoding=encoding, index=retain_index) # 保存到文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请求头 这里我尽量弄的全一点\n",
    "headers = {\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language':'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "    'cookie': 'pgv_pvid=7838988661; ptcz=a749c13713fc703b48592a49922470ed1eda1bcc7b68d2cf5be6fa79c731b76d; pgv_pvi=9718182912; RK=vkrweAJtMM; o_cookie=763074310; pac_uid=1_763074310; luin=o0763074310; lskey=000100007d08c463d9cc757cc4c5ae2cf3732897a65bf7de58e88b9524158f61b02e143bcf6124869366ea7d; tvfe_boss_uuid=e1c916f5e5e6e6bd; _txjk_whl_uuid_aa5wayli=58f307f1d8264ccb8aeb92b2f6225b14',\n",
    "    'referer': 'https://news.qq.com/zt2020/page/feiyan.htm',\n",
    "    'sec-fetch-dest': 'script',\n",
    "    'sec-fetch-mode': 'no-cors',\n",
    "    'sec-fetch-site': 'same-site',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36 Edg/83.0.478.61'\n",
    "}\n",
    "# 由于网址不同需要的请求头也不太一样，故设置两个请求头，注意cookie是一段时间就会改变的，需要手动来改\n",
    "headers2 = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Cookie': 'pgv_pvid=7838988661; ptcz=a749c13713fc703b48592a49922470ed1eda1bcc7b68d2cf5be6fa79c731b76d; pgv_pvi=9718182912; RK=vkrweAJtMM; o_cookie=763074310; pac_uid=1_763074310; luin=o0763074310; lskey=000100007d08c463d9cc757cc4c5ae2cf3732897a65bf7de58e88b9524158f61b02e143bcf6124869366ea7d; tvfe_boss_uuid=e1c916f5e5e6e6bd; _txjk_whl_uuid_aa5wayli=58f307f1d8264ccb8aeb92b2f6225b14; uin=o0763074310; skey=@xEkmEQkwn; qqmusic_uin=; qqmusic_key=; qqmusic_fromtag=; pgv_info=ssid=s1515695283',\n",
    "    'Host': 'api.inews.qq.com',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36 Edg/83.0.478.61'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 所需要的所有url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 国内数据\n",
    "url0 = \"https://view.inews.qq.com/g2/getOnsInfo?name=disease_h5\"\n",
    " # 国内数据（随时间变化）\n",
    "url1 = \"https://view.inews.qq.com/g2/getOnsInfo?name=disease_other\"\n",
    "# 外国数据（比率和排名）\n",
    "url2 = \"https://api.inews.qq.com/newsqa/v1/automation/modules/list?modules=FAutoCountryWeekCompRank,FAutoContinentConfirmStatis,FAutoConfirmMillionRankList,FAutoHealDeadRateRankList\"\n",
    "# 外国数据（排名）\n",
    "url3 = \"https://view.inews.qq.com/g2/getOnsInfo?name=disease_foreign\"\n",
    "# 全球总体数据（随时间变化）\n",
    "url4 = \"https://api.inews.qq.com/newsqa/v1/automation/modules/list?modules=FAutoGlobalStatis,FAutoContinentStatis,FAutoGlobalDailyList,FAutoCountryConfirmAdd\"\n",
    "# 外国数据（按国家或大洲查看）\n",
    "url5 = \"https://api.inews.qq.com/newsqa/v1/automation/foreign/country/ranklist\"\n",
    "# 外国部分国家历史每日数据（随时间变化）\n",
    "url6 = \"https://api.inews.qq.com/newsqa/v1/automation/foreign/daily/list?country=\"\n",
    "# 国内具体省市每日数据\n",
    "url7 = \"https://api.inews.qq.com/newsqa/v1/query/pubished/daily/list?province=\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 国内数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_china_data(url, headers)\n",
    "    # url0 = 'https://view.inews.qq.com/g2/getOnsInfo?name=disease_h5\n",
    "    res0 = requests.get(url, headers=headers) # 请求数据，使用第一种请求头\n",
    "    data0 = json.loads(res0.json()['data']) # 因为data对应的值还是json字符串，还要转成Python字典\n",
    "\n",
    "\n",
    "    china_data = {'name': data0['areaTree'][0]['name']} # 新建一个字典，并地区名添加进去\n",
    "    china_data.update(data0['areaTree'][0]['total']) # 合并地区的数据，字典合并字典\n",
    "    provinces_data =  [china_data] # 先创建省份数据列表，后续数据直接追加尽量，列表中的数据都是字典，先把全国的数据添进去\n",
    "\n",
    "    # 各省份的数据都在 data['areaTree'][0]['children']这里，这是个列表，34个省级行政区\n",
    "    provinces = data0['areaTree'][0]['children']\n",
    "    # 遍历这个列表取出每个省的数据和每个地区的数据\n",
    "    for i in provinces:\n",
    "        # i['total'] 就是每个省的数据，里面的 showRate 和 showHeal 字段是不需要的 等同于这样data['areaTree'][0]['children'][0]['total']\n",
    "        one_procevice_data = {'name': i['name']} # 将省份的名字先添加进字典\n",
    "        one_procevice_data.update(i['total']) # 合并具体数据 i['total']是字典型的数据\n",
    "        provinces_data.append(one_procevice_data) # 向省份列表追加每个省的数据\n",
    "\n",
    "        # 获取省内的各市的数据 省内的数据一次就保存到单独的csv文件中\n",
    "        districts = i['children']\n",
    "        districts_data = [one_procevice_data]\n",
    "        for j in districts:\n",
    "            one_district_data = {'name': j['name']}\n",
    "            one_district_data.update(j['total'])\n",
    "            districts_data.append(one_district_data)\n",
    "\n",
    "        # 将字典构成的列表直接转为DataFrame\n",
    "        districts_df = pd.DataFrame(districts_data)\n",
    "        del districts_df['showRate'] # 删除这多余的两列\n",
    "        del districts_df['showHeal']\n",
    "\n",
    "        # 保存这个省内各市的数据\n",
    "        # districts_df.rename(columns=field_name_cn, inplace=True) # 把列名改为中文\n",
    "        df_to_csv(districts_df, filepath_cn+'/各省具体数据', f'{i[\"name\"]}.csv') # 存储各省具体数据\n",
    "        # districts_df.to_csv(f\"{filepath}/{i['name']}.csv\", encoding='GBK', retain_index=False) # 保存到文件夹中 \n",
    "\n",
    "\n",
    "    # 将字典构成的列表直接转为DataFrame\n",
    "    provinces_df = pd.DataFrame(provinces_data)                        \n",
    "    del provinces_df['showRate'] # 删除这多余的两列\n",
    "    del provinces_df['showHeal']\n",
    "\n",
    "    # 保存每个省的数据\n",
    "    # provinces_df.rename(columns=field_name_cn, inplace=True) # 把列名改为中文\n",
    "    df_to_csv(provinces_df, filepath_cn, '中国疫情数据.csv')\n",
    "    # provinces_df.to_csv(f\"{field_name_cn}/中国.csv\", encoding='GBK', retain_index=False) # 保存到文件夹中 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 国内数据（随时间变化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_china_time_data(url, headers):\n",
    "    # url1 = 'https://view.inews.qq.com/g2/getOnsInfo?name=disease_other' # 国内数据（随时间变化）\n",
    "    res1 = requests.get(url, headers=headers)\n",
    "    data1 = json.loads(res1.json()['data']) # 因为data对应的值还是json字符串，还要转成Python字典\n",
    "    \n",
    "    # 该部分可以直接转DataFrame然后存储数据\n",
    "    df_to_csv(pd.DataFrame(data1['chinaDayList']), filepath_cn, '国内历史每日数据.csv')\n",
    "    df_to_csv(pd.DataFrame(data1['chinaDayAddList']), filepath_cn, '国内历史每日新增数据.csv')\n",
    "    df_to_csv(pd.DataFrame(data1['dailyNewAddHistory']), filepath_cn, '历史每日新增数据（湖北、国家、和非湖北）.csv')\n",
    "    \n",
    "    prov_compare = pd.DataFrame(data1['provinceCompare']) # 各省数据比较（现存确诊，新增确诊，死亡，治愈，连续0增天数\n",
    "    prov_compare = pd.DataFrame(prov_compare.values.T, index=prov_compare.columns, columns=prov_compare.index) # 转置\n",
    "    df_to_csv(prov_compare, filepath_cn, '各省今日数据比较.csv', retain_index=True) # 保留索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 外国数据（比率和排名）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreign_rate_rank(url, headers):\n",
    "    # url2 = 'https://api.inews.qq.com/newsqa/v1/automation/modules/list?modules=FAutoCountryWeekCompRank,FAutoContinentConfirmStatis,FAutoConfirmMillionRankList,FAutoHealDeadRateRankList'\n",
    "    res2 = requests.get(url, headers=headers) # 这里用了headers2\n",
    "    data2 = res2.json()['data']\n",
    "    \n",
    "    # 存储数据直接用 二次包装的to_csv方法 这里并没有修改列名为中文\n",
    "    df_to_csv(pd.DataFrame(data2['FAutoConfirmMillionRankList']), filepath_fg, '外国每百万人确诊数.csv')\n",
    "    df_to_csv(pd.DataFrame(data2['FAutoContinentConfirmStatis']), filepath_fg, '外国累计确诊周增幅.csv')\n",
    "    df_to_csv(pd.DataFrame(data2['FAutoCountryWeekCompRank']), filepath_fg, '外国累计确诊七日增幅.csv')\n",
    "    df_to_csv(pd.DataFrame(data2['FAutoHealDeadRateRankList']['deadHead']), filepath_fg, '外国死亡率前10名.csv')\n",
    "    df_to_csv(pd.DataFrame(data2['FAutoHealDeadRateRankList']['deadTail']), filepath_fg, '外国死亡率后10名.csv')\n",
    "    df_to_csv(pd.DataFrame(data2['FAutoHealDeadRateRankList']['healHead']), filepath_fg, '外国治愈前10名.csv')\n",
    "    df_to_csv(pd.DataFrame(data2['FAutoHealDeadRateRankList']['healTail']), filepath_fg, '外国治愈后10名.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 外国数据（排名和部分国家具体行政区数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreign_rank_region(url, headers)\n",
    "    # url3 = 'https://view.inews.qq.com/g2/getOnsInfo?name=disease_foreign' # 外国数据（排名）\n",
    "    res3 = requests.get(url, headers=headers)\n",
    "    data3 = json.loads(res3.json()['data']) # 因为data对应的值还是json字符串，还要转成Python字典\n",
    "    \n",
    "    # 存储数据\n",
    "    df_to_csv(pd.DataFrame(data3['importStatis']['TopList']), filepath_cn, '国内输入病例前十省.csv')\n",
    "    df_to_csv(pd.DataFrame(data3['countryAddConfirmRankList']), filepath_fg, '外国24小时新增确诊前10名.csv')\n",
    "    df_to_csv(pd.DataFrame(data3['countryConfirmWeekCompareRankList']), filepath_fg, '外国七天变化排名.csv')\n",
    "    \n",
    "    # 存储部分国家的具体行政区数据数据  不过这里面就几个国家，同\n",
    "    for i in data3['foreignList']: # i为国家的数据\n",
    "        # 先判断这个国家有没有具体类似我国省这样的行政区的数据，即判断有没有 children 这个字段\n",
    "        if 'children' in i: # 等同于 in i.keys()\n",
    "            # 当存在时，可直接转换成DataFrame并存储\n",
    "            df_to_csv(pd.DataFrame(i['children']), filepath_fg+'/部分国家具体行政区数据', f\"{i['name']}-具体行政区数据.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全球总体数据（随时间变化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreign_time_data(url, headers):\n",
    "    # url4 = \"https://api.inews.qq.com/newsqa/v1/automation/modules/list?modules=FAutoGlobalStatis,FAutoContinentStatis,FAutoGlobalDailyList,FAutoCountryConfirmAdd\"\n",
    "    res4 = requests.get(url, headers=headers) # 这里用了headers2\n",
    "    data4 = res4.json()['data']\n",
    "    \n",
    "    for i in data4['FAutoContinentStatis']: # 各大洲历史每七天的数据统计\n",
    "        i.update(i.pop('statis')) # 把statis弹出再追加进去就可以直接转成DataFrame\n",
    "    # 存储数据\n",
    "    df_to_csv(pd.DataFrame(data4['FAutoContinentStatis']), filepath_fg, '各大洲历史每七天的数据统计.csv')\n",
    "    \n",
    "    global_daily_data = [] # 存储全球每日数据的列表\n",
    "    for i in data4['FAutoGlobalDailyList']:\n",
    "        day_data = {'date': i['date']}\n",
    "        day_data.update(i['all']) # 就all字段中的每日数据追加进去\n",
    "        global_daily_data.append(day_data)\n",
    "    # 存储数据\n",
    "    df_to_csv(pd.DataFrame(global_daily_data), filepath_fg, '全球历史每日数据.csv') # 转换成DataFrame，并存储到本地"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 外国数据（按国家或大洲查看）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreign_country_continent(url, headers):\n",
    "    # url5 = \"https://api.inews.qq.com/newsqa/v1/automation/foreign/country/ranklist\"\n",
    "    res5 = requests.get(url, headers=headers)\n",
    "    data5 = res5.json()['data']\n",
    "    \n",
    "    # 存储\n",
    "    df_to_csv(pd.DataFrame(data5), filepath_fg, '外国疫情数据（按国家或大洲查看）.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 外国部分国家历史每日数据（随时间变化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美国 每日数据爬取OK\n",
      "巴西 每日数据爬取OK\n",
      "印度 每日数据爬取OK\n",
      "俄罗斯 每日数据爬取OK\n",
      "秘鲁 每日数据爬取OK\n",
      "智利 每日数据爬取OK\n",
      "墨西哥 每日数据爬取OK\n",
      "西班牙 每日数据爬取OK\n",
      "南非 每日数据爬取OK\n",
      "英国 每日数据爬取OK\n",
      "伊朗 每日数据爬取OK\n",
      "巴基斯坦 每日数据爬取OK\n",
      "意大利 每日数据爬取OK\n",
      "沙特阿拉伯 每日数据爬取OK\n",
      "土耳其 每日数据爬取OK\n",
      "法国 每日数据爬取OK\n",
      "德国 每日数据爬取OK\n",
      "孟加拉 每日数据爬取OK\n",
      "哥伦比亚 每日数据爬取OK\n",
      "加拿大 每日数据爬取OK\n",
      "阿根廷 每日数据爬取OK\n",
      "卡塔尔 每日数据爬取OK\n",
      "埃及 每日数据爬取OK\n",
      "伊拉克 每日数据爬取OK\n",
      "印度尼西亚 每日数据爬取OK\n",
      "瑞典 每日数据爬取OK\n",
      "厄瓜多尔 每日数据爬取OK\n",
      "白俄罗斯 每日数据爬取OK\n",
      "比利时 每日数据爬取OK\n",
      "哈萨克斯坦 每日数据爬取OK\n",
      "阿曼 每日数据爬取OK\n",
      "菲律宾 每日数据爬取OK\n",
      "科威特 每日数据爬取OK\n",
      "乌克兰 每日数据爬取OK\n",
      "阿联酋 每日数据爬取OK\n",
      "荷兰 每日数据爬取OK\n",
      "玻利维亚 每日数据爬取OK\n",
      "巴拿马 每日数据爬取OK\n",
      "葡萄牙 每日数据爬取OK\n",
      "新加坡 每日数据爬取OK\n",
      "多米尼加 每日数据爬取OK\n",
      "以色列 每日数据爬取OK\n",
      "波兰 每日数据爬取OK\n",
      "阿富汗 每日数据爬取OK\n",
      "巴林 每日数据爬取OK\n",
      "尼日利亚 每日数据爬取OK\n",
      "罗马尼亚 每日数据爬取OK\n",
      "瑞士 每日数据爬取OK\n",
      "亚美尼亚 每日数据爬取OK\n",
      "危地马拉 每日数据爬取OK\n",
      "洪都拉斯 每日数据爬取OK\n",
      "爱尔兰 每日数据爬取OK\n",
      "阿塞拜疆 每日数据爬取OK\n",
      "加纳 每日数据爬取OK\n",
      "日本本土 每日数据爬取OK\n",
      "阿尔及利亚 每日数据爬取OK\n",
      "摩尔多瓦 每日数据爬取OK\n",
      "奥地利 每日数据爬取OK\n",
      "塞尔维亚 每日数据爬取OK\n",
      "尼泊尔 每日数据爬取OK\n",
      "摩洛哥 每日数据爬取OK\n",
      "喀麦隆 每日数据爬取OK\n",
      "乌兹别克斯坦 每日数据爬取OK\n",
      "韩国 每日数据爬取OK\n",
      "捷克 每日数据爬取OK\n",
      "丹麦 每日数据爬取OK\n",
      "科特迪瓦 每日数据爬取OK\n",
      "吉尔吉斯斯坦 每日数据爬取OK\n",
      "肯尼亚 每日数据爬取OK\n",
      "澳大利亚 每日数据爬取OK\n",
      "苏丹 每日数据爬取OK\n",
      "萨尔瓦多 每日数据爬取OK\n",
      "委内瑞拉 每日数据爬取OK\n",
      "挪威 每日数据爬取OK\n",
      "马来西亚 每日数据爬取OK\n",
      "哥斯达黎加 每日数据爬取OK\n",
      "北马其顿 每日数据爬取OK\n",
      "塞内加尔 每日数据爬取OK\n",
      "刚果（金） 每日数据爬取OK\n",
      "埃塞俄比亚 每日数据爬取OK\n",
      "保加利亚 每日数据爬取OK\n",
      "芬兰 每日数据爬取OK\n",
      "波黑 每日数据爬取OK\n",
      "巴勒斯坦 每日数据爬取OK\n",
      "海地 每日数据爬取OK\n",
      "塔吉克斯坦 每日数据爬取OK\n",
      "几内亚 每日数据爬取OK\n",
      "加蓬 每日数据爬取OK\n",
      "毛里塔尼亚 每日数据爬取OK\n",
      "马达加斯加 每日数据爬取OK\n",
      "卢森堡 每日数据爬取OK\n",
      "吉布提 每日数据爬取OK\n",
      "中非共和国 每日数据爬取OK\n",
      "匈牙利 每日数据爬取OK\n",
      "希腊 每日数据爬取OK\n",
      "克罗地亚 每日数据爬取OK\n",
      "阿尔巴尼亚 每日数据爬取OK\n",
      "泰国 每日数据爬取OK\n",
      "尼加拉瓜 每日数据爬取OK\n",
      "索马里 每日数据爬取OK\n",
      "巴拉圭 每日数据爬取OK\n",
      "赤道几内亚 每日数据爬取OK\n",
      "马尔代夫 每日数据爬取OK\n",
      "斯里兰卡 每日数据爬取OK\n",
      "马拉维 每日数据爬取OK\n",
      "黎巴嫩 每日数据爬取OK\n",
      "古巴 每日数据爬取OK\n",
      "马里 每日数据爬取OK\n",
      "刚果（布） 每日数据爬取OK\n",
      "爱沙尼亚 每日数据爬取OK\n",
      "斯洛伐克 每日数据爬取OK\n",
      "冰岛 每日数据爬取OK\n",
      "赞比亚 每日数据爬取OK\n",
      "立陶宛 每日数据爬取OK\n",
      "斯洛文尼亚 每日数据爬取OK\n",
      "几内亚比绍 每日数据爬取OK\n",
      "佛得角 每日数据爬取OK\n",
      "塞拉利昂 每日数据爬取OK\n",
      "利比亚 每日数据爬取OK\n",
      "新西兰 每日数据爬取OK\n",
      "也门 每日数据爬取OK\n",
      "斯威士兰 每日数据爬取OK\n",
      "卢旺达 每日数据爬取OK\n",
      "贝宁 每日数据爬取OK\n",
      "突尼斯 每日数据爬取OK\n",
      "黑山 每日数据爬取OK\n",
      "莫桑比克 每日数据爬取OK\n",
      "约旦 每日数据爬取OK\n",
      "拉脱维亚 每日数据爬取OK\n",
      "尼日尔 每日数据爬取OK\n",
      "津巴布韦 每日数据爬取OK\n",
      "乌干达 每日数据爬取OK\n",
      "布基纳法索 每日数据爬取OK\n",
      "利比里亚 每日数据爬取OK\n",
      "塞浦路斯 每日数据爬取OK\n",
      "格鲁吉亚 每日数据爬取OK\n",
      "乌拉圭 每日数据爬取OK\n",
      "乍得 每日数据爬取OK\n",
      "纳米比亚 每日数据爬取OK\n",
      "安道尔 每日数据爬取OK\n",
      "苏里南 每日数据爬取OK\n",
      "牙买加 每日数据爬取OK\n",
      "多哥 每日数据爬取OK\n",
      "钻石号邮轮 每日数据爬取OK\n",
      "圣马力诺 每日数据爬取OK\n",
      "马耳他 每日数据爬取OK\n",
      "安哥拉 每日数据爬取OK\n",
      "坦桑尼亚 每日数据爬取OK\n",
      "叙利亚 每日数据爬取OK\n",
      "博茨瓦纳 每日数据爬取OK\n",
      "越南 每日数据爬取OK\n",
      "毛里求斯 每日数据爬取OK\n",
      "缅甸 每日数据爬取OK\n",
      "科摩罗 每日数据爬取OK\n",
      "圭亚那 每日数据爬取OK\n",
      "布隆迪 每日数据爬取OK\n",
      "莱索托 每日数据爬取OK\n",
      "蒙古 每日数据爬取OK\n",
      "厄立特里亚 每日数据爬取OK\n",
      "柬埔寨 每日数据爬取OK\n",
      "文莱 每日数据爬取OK\n",
      "特立尼达和多巴哥 每日数据爬取OK\n",
      "巴哈马 每日数据爬取OK\n",
      "摩纳哥 每日数据爬取OK\n",
      "巴巴多斯 每日数据爬取OK\n",
      "塞舌尔 每日数据爬取OK\n",
      "列支敦士登公国 每日数据爬取OK\n",
      "不丹 每日数据爬取OK\n",
      "安提瓜和巴布达 每日数据爬取OK\n",
      "冈比亚 每日数据爬取OK\n",
      "伯利兹 每日数据爬取OK\n",
      "马提尼克岛 每日数据爬取OK\n",
      "圣文森特和格林纳丁斯 每日数据爬取OK\n",
      "斐济 每日数据爬取OK\n",
      "东帝汶 每日数据爬取OK\n",
      "格林纳达 每日数据爬取OK\n",
      "圣卢西亚 每日数据爬取OK\n",
      "老挝 每日数据爬取OK\n",
      "多米尼克 每日数据爬取OK\n",
      "圣基茨和尼维斯 每日数据爬取OK\n",
      "梵蒂冈 每日数据爬取OK\n",
      "巴布亚新几内亚 每日数据爬取OK\n"
     ]
    }
   ],
   "source": [
    "def get_foreign_day_time_data(url, headers):\n",
    "    # url6 = \"https://api.inews.qq.com/newsqa/v1/automation/foreign/daily/list?country=\"\n",
    "\n",
    "    # 这里就会用到data5，其中name字段就是国家名\n",
    "    country_list = [c['name'] for c in data5] # 获取国家列表\n",
    "    for i in country_list:  \n",
    "        # 请求每个国家的数据，这里并不是所有的国家都有统计每日数据\n",
    "        # 因此最终只能得到部分国家的数据\n",
    "        res6 = requests.get(url+i, headers=headers) \n",
    "        data6 = res6.json()['data'] # 提取具体数据\n",
    "\n",
    "        if data6: # 当该国有数据的时候就存储下来\n",
    "            # 可直接存储数据\n",
    "            df_to_csv(pd.DataFrame(data6), filepath_fg+'/部分国家历史每日数据', f'{i}.csv')\n",
    "            print(f\"{i} 每日数据爬取OK\")\n",
    "\n",
    "        # 这里由于频繁请求，怕被封因此睡眠1秒\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 国内具体省市的每日数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "安徽 每日数据爬取OK\n",
      "澳门 每日数据爬取OK\n",
      "香港 每日数据爬取OK\n",
      "北京 每日数据爬取OK\n",
      "福建 每日数据爬取OK\n",
      "甘肃 每日数据爬取OK\n",
      "广东 每日数据爬取OK\n",
      "广西 每日数据爬取OK\n",
      "贵州 每日数据爬取OK\n",
      "海南 每日数据爬取OK\n",
      "河北 每日数据爬取OK\n",
      "河南 每日数据爬取OK\n",
      "黑龙江 每日数据爬取OK\n",
      "湖北 每日数据爬取OK\n",
      "湖南 每日数据爬取OK\n",
      "吉林 每日数据爬取OK\n",
      "江苏 每日数据爬取OK\n",
      "江西 每日数据爬取OK\n",
      "辽宁 每日数据爬取OK\n",
      "内蒙古 每日数据爬取OK\n",
      "宁夏 每日数据爬取OK\n",
      "青海 每日数据爬取OK\n",
      "山东 每日数据爬取OK\n",
      "山西 每日数据爬取OK\n",
      "陕西 每日数据爬取OK\n",
      "上海 每日数据爬取OK\n",
      "四川 每日数据爬取OK\n",
      "台湾 每日数据爬取OK\n",
      "天津 每日数据爬取OK\n",
      "西藏 每日数据爬取OK\n",
      "香港 每日数据爬取OK\n",
      "新疆 每日数据爬取OK\n",
      "云南 每日数据爬取OK\n",
      "浙江 每日数据爬取OK\n",
      "重庆 每日数据爬取OK\n"
     ]
    }
   ],
   "source": [
    "def get_china_district_day_data(url, headers):\n",
    "    # url7 = \"https://api.inews.qq.com/newsqa/v1/query/pubished/daily/list?province=\"\n",
    "    filepath_cn = \"./疫情数据/国内数据\"\n",
    "    provinces = ['安徽', '澳门', '香港', '北京', '福建', '甘肃', '广东', '广西', '贵州', '海南', '河北', '河南', '黑龙江', '湖北', '湖南', '吉林', '江苏', '江西', '辽宁', '内蒙古', '宁夏', '青海', '山东', '山西', '陕西', '上海', '四川', '台湾', '天津', '西藏', '香港', '新疆', '云南', '浙江', '重庆']\n",
    "    for p in provinces:\n",
    "        res7 = requests.get(url+p, headers=headers)\n",
    "        data7 = res7.json()['data'] # 提取具体数据\n",
    "\n",
    "        df_to_csv(pd.DataFrame(data7), filepath_cn+'/各省每日数据', f'{p}.csv')\n",
    "        print(f\"{p} 每日数据爬取OK\")\n",
    "\n",
    "        # 这里由于频繁请求，怕被封因此睡眠1秒\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行爬虫\n",
    "get_china_data(url0, headers)\n",
    "get_china_time_data(url1, headers)\n",
    "get_foreign_rate_rank(url2, headers2)\n",
    "get_foreign_rank_region(url3, headers)\n",
    "get_foreign_time_data(url4, headers2)\n",
    "get_foreign_country_continent(url5, headers)\n",
    "get_foreign_day_time_data(url6, headers2)\n",
    "get_china_district_day_data(url7, headers2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
